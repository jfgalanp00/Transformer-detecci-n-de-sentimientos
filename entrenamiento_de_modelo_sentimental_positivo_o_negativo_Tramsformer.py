# -*- coding: utf-8 -*-
"""Entrenamiento de modelo sentimental positivo o negativo

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14QWdfY1IgMS0oZQRHOYpDnZVQhNL03jC
"""

!pip install transformers

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from transformers import AdamW
from sklearn.model_selection import train_test_split
import pandas as pd

# Cargar el modelo preentrenado y el tokenizador
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Cargar el dataset IMDB Movie Review
df = pd.read_csv("imdb_reviews.csv",sep='.  	')

df

# Dividir el dataset en conjunto de entrenamiento y conjunto de evaluación
train_texts, eval_texts, train_labels, eval_labels = train_test_split(df['review'].tolist(), df['label'].tolist(), test_size=0.2)

# Establecer la longitud máxima deseada de las secuencias
max_length = 128

# Realizar el padding en los datos de entrenamiento
train_input_ids = []
train_attn_masks = []

for texto in train_texts:
    tokens = tokenizer.tokenize(texto)
    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]
    input_id = tokenizer.convert_tokens_to_ids(tokens)
    attn_mask = [1] * len(input_id)

    # Aplicar el padding si es necesario
    if len(input_id) < max_length:
        padding_length = max_length - len(input_id)
        input_id = input_id + [tokenizer.pad_token_id] * padding_length
        attn_mask = attn_mask + [0] * padding_length

    train_input_ids.append(input_id)
    train_attn_masks.append(attn_mask)

# Convertir los datos de entrenamiento a tensores de PyTorch
train_input_ids = torch.tensor(train_input_ids)
train_attn_masks = torch.tensor(train_attn_masks)
train_labels = torch.tensor(train_labels)

# Crear el DataLoader de entrenamiento
batch_size = 16
train_dataset = TensorDataset(train_input_ids, train_attn_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Definir los hiperparámetros de entrenamiento
lr = 2e-5
epochs = 3

# Definir el optimizador y la función de pérdida
optimizer = AdamW(model.parameters(), lr=lr)
criterion = torch.nn.CrossEntropyLoss()

# Entrenamiento del modelo
model.train()

for epoch in range(epochs):
    total_loss = 0

    for batch in train_dataloader:
        batch_input_ids, batch_attn_masks, batch_labels = batch

        # Realizar el paso forward y obtener las predicciones
        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attn_masks)
        logits = outputs.logits

        # Calcular la pérdida
        loss = criterion(logits, batch_labels)
        total_loss += loss.item()

        # Realizar el paso backward y actualizar los parámetros del modelo
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Calcular la pérdida promedio por época
    avg_loss = total_loss / len(train_dataloader)
    print(f"Época {epoch + 1}: Pérdida = {avg_loss:.4f}")

# Evaluación del modelo
model.eval()

eval_input_ids = []
eval_attn_masks = []

for texto in eval_texts:
    tokens = tokenizer.tokenize(texto)
    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]
    input_id = tokenizer.convert_tokens_to_ids(tokens)
    attn_mask = [1] * len(input_id)

    eval_input_ids.append(input_id)
    eval_attn_masks.append(attn_mask)

eval_input_ids = torch.tensor(eval_input_ids)
eval_attn_masks = torch.tensor(eval_attn_masks)
eval_labels = torch.tensor(eval_labels)

eval_dataset = TensorDataset(eval_input_ids, eval_attn_masks, eval_labels)
eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)

total_accuracy = 0

for batch in eval_dataloader:
    batch_input_ids, batch_attn_masks, batch_labels = batch

    with torch.no_grad():
        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attn_masks)
        logits = outputs.logits

    # Calcular la precisión
    _, predicted_labels = torch.max(logits, dim=1)
    accuracy = (predicted_labels == batch_labels).float().mean().item()
    total_accuracy += accuracy

avg_accuracy = total_accuracy / len(eval_dataloader)
print(f"Precisión en el conjunto de evaluación: {avg_accuracy:.4f}")

# Guardar el modelo entrenado
output_dir = "./modelo_entrenado/"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

